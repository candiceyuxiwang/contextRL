---
title: "Prolific_data_analysis.rmd"
author: "Candice Wang"
date: "1/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### read in data

```{r}
library(dplyr)
library(ggplot2)
learn_data <- read.csv("/Volumes/main2/studies/Context_RL/Data/context_RL_learn.csv")

# obtain reward list info based on reward sequence
learn_data$reward_list <- NA
for (sub in 1:length(unique(learn_data$ID))){
  if(as.numeric(as.factor(unique(learn_data$ID))[sub]) < 39){ # only 92 trials for this subject
      start_ind <- 100*(as.numeric(as.factor(unique(learn_data$ID))[sub])-1)+1
      end_ind <- 100*as.numeric(as.factor(unique(learn_data$ID))[sub])
  }else if(as.numeric(as.factor(unique(learn_data$ID))[sub]) == 39){
      start_ind <- 100*(as.numeric(as.factor(unique(learn_data$ID))[sub])-1)+1
      end_ind <- 100*as.numeric(as.factor(unique(learn_data$ID))[sub]) - 8
  }else{
      start_ind <- 100*(as.numeric(as.factor(unique(learn_data$ID))[sub])-1)+1 -8
      end_ind <- 100*as.numeric(as.factor(unique(learn_data$ID))[sub]) -8
  }
  if(learn_data$blue_val[learn_data$learn_trial_n==1][as.numeric(as.factor(unique(learn_data$ID))[sub])] == 37.634){
    learn_data$reward_list[start_ind:end_ind] <- 3
  }else if(learn_data$blue_val[learn_data$learn_trial_n==1][as.numeric(as.factor(unique(learn_data$ID))[sub])] == 39.894){
    learn_data$reward_list[start_ind:end_ind] <- 1
  }else{
    learn_data$reward_list[start_ind:end_ind] <- 2
  }
}

#convert door choice into numeric variable
### door numbers
door_key <- c("red","blue","purple","yellow")
learn_data <- learn_data %>%
    mutate(chosen_door_num = case_when(
      grepl(door_key[1],chosen_door) ~ 1,
      grepl(door_key[2],chosen_door) ~ 2,
      grepl(door_key[3],chosen_door) ~ 3,
      grepl(door_key[4],chosen_door) ~ 4
    ))
learn_data$ID_num <- as.numeric(as.factor(learn_data$ID))
```

### % optimal choice as function of time on task

```{r}
bin_size = 10
learn_data %>%
  mutate(trial_bin = case_when(
    learn_trial_n < bin_size + 1 ~ 1,
    learn_trial_n < bin_size*2 + 1 ~ 2,
    learn_trial_n < bin_size*3 + 1 ~ 3,
    learn_trial_n < bin_size*4 + 1 ~ 4,
    learn_trial_n < bin_size*5 + 1 ~ 5,
    learn_trial_n < bin_size*6 + 1 ~ 6,
    learn_trial_n < bin_size*7 + 1 ~ 7,
    learn_trial_n < bin_size*8 + 1 ~ 8,
    learn_trial_n < bin_size*9 + 1 ~ 9,
    TRUE ~ 10
  )) %>%
  group_by(ID, trial_bin, condition) %>%
  summarise(p_optimal = sum(optimal_choice)/bin_size)%>%
  mutate(trial_bin = as.factor(trial_bin))%>%
  ggplot(aes(x = trial_bin, y = p_optimal))+
  geom_boxplot(aes(color = condition))
  
```

## model-based analysis of choice data

We can try fitting the model with a delta learning rule for reward updating.

We expect that participants in the interrogative condition would have greater directed exploration compared to participants in the imperative condition, while the other parameters would not necessarily differ. We can model them as two separate populations and estimate the *difference* in directed exploration, random exploration, and perserveration free parameters between the two groups.

```{r}
library(rstan)
rstan_options(javascript=FALSE)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
# compute exploration bonus for delta learning rule
exploration_bonus <- data.frame(matrix(nrow = nrow(learn_data), ncol = 4))
for (t in 1:(nrow(learn_data)-1)){
  if(learn_data$learn_trial_n[t]==1){
    exploration_bonus[t,] <- 1
  }
  for (i in 1:4){
     # for each arm, see check the last time that it was chosen
      if(length(which(learn_data$chosen_door_num[(1+(learn_data$ID_num[t]-1)*100):t] %in% i))==0){
        exploration_bonus[t+1,i] <- t - (learn_data$ID_num[t]-1)*100 # hasn't been chosen before
      }else{
        exploration_bonus[t+1,i] <- t - (max(which(learn_data$chosen_door_num[(1+(learn_data$ID_num[t]-1)*100):t] %in% i)) + (learn_data$ID_num[t]-1)*100)
      }
  }
}

# make data lists for stan
delta_data_list  <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward,
             eb = exploration_bonus
             ) 
bayes_data_list <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward
)

# with condition information
learn_data_list_delta <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward,
             eb = exploration_bonus, 
             condition = as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1]))==1)
             ) 
bayes_data_list_1 <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward,
             condition = as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1]))
)
```

fit observed data to the simple delta learning model (no exploration bonus or perserveration):

```{r delta_SM}
learn_data_list_delta_SM <- list(
             totalTrials = nrow(learn_data),
             nSubjects = length(unique(learn_data$ID)),
             subject = learn_data$ID_num,
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num,
             rewards = learn_data$reward,
             condition = as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1]))==1)
             )
# run on Cerberus

learn_fit_delta_SM_cond_diffpriors <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/learn_fit_delta_SM_cond_diffpriors.rds")

print(learn_fit_delta_SM_cond_diffpriors, pars = c("eta_mu","eta_sigma","alpha_mu", "alpha_sigma","beta_mu","beta_sigma", "alpha_mu_diff", "beta_mu_diff","eta_mu_diff"))
```

This model works, both with tighter priors and more diffuse priors (delta_SM_cond_diffpriors.stan). fitted objects are saved on cerberus.

Now, we can look at the estimated group differences in learning rate and inverse temperature:

```{r}
plot(learn_fit_delta_SM_cond_diffpriors, pars = c("alpha_mu_diff","beta_mu_diff"))
```

At the group level, participants in the imperative condition had higher learning rates than those in the interrogative condition.

Now, we can add in another free parameter for the directed exploration bonus. 

```{r delta_SME}

```

### classify choice type based on model-derived expected values

We can classify whether a trial is exploitative or exploratory based on the model-estiamted Q value, and whether it is directed exploration (choosing the choice with the most uncertainty) or random exploration based on choice history.

```{r}
# extract Q value
delta_SM_Q_summary <- summary(learn_fit_delta_SM_cond_diffpriors, pars = c("Q"), probs = c(0.1, 0.9))$summary
delta_SM_Q_summary <- delta_SM_Q_summary[,"mean"]
delta_SM_Q_summary <- matrix(delta_SM_Q_summary, nrow = nrow(learn_data), byrow = T)
delta_SM_Q_summary <- as.data.frame(delta_SM_Q_summary)
delta_SM_Q_summary <- delta_SM_Q_summary %>%
  cbind(max_Q_choice = max.col(delta_SM_Q_summary, 'first'))

# append columns needed 
exploration_bonus <- exploration_bonus %>%
  cbind(dir_expl_choice = max.col(exploration_bonus, 'first'))
exploration_bonus$learn_trial_n <- learn_data$learn_trial_n
exploration_bonus$subject <- learn_data$ID_num
exploration_bonus$condition <- as.factor(learn_data$condition)
exploration_bonus$reward_list <- as.factor(learn_data$reward_list)
  
# classify choice on each trial 
choice_classification <- exploration_bonus %>%
  cbind(choice = learn_data$chosen_door_num) %>%
  cbind(delta_SM_Q_summary) %>%
  mutate(choice_type = case_when(
    choice == max_Q_choice ~ "exploitation",
    choice == dir_expl_choice ~ "directed_exploration",
    TRUE ~ "random_exploration"
  ),choice_type = as.factor(choice_type))

# plot
choice_classification %>%
  group_by(subject, condition) %>%
  count(choice_type) %>%
  ungroup() %>%
  group_by(condition) %>%
  ggplot(aes(x = condition, y = n, color = choice_type))+
  geom_boxplot()+
  stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="pointrange", position = position_dodge(0.75))

# some stats
library(mclogit)
mod <- mblogit(choice_type ~ condition + reward_list, data = choice_classification, random = ~1|subject)
summary(mod)

```


Based on the the multinomial logistic regression, the interrogative group made more directed exploration choices to resolve uncertainty compared to the imperative group, and the imperative group made more exploitation choices to maximize points gained.


```{r}
# library(cmdstanr)
# Bayes_SMEP_mod <- cmdstan_model("Bayes_SMEP_cond.stan")
# 
# fit_Bayes_SMEP <- Bayes_SMEP_mod$sample(
#   data = bayes_data_list_1, 
#   chains = 2, 
#   parallel_chains = 2,
#   iter_warmup = 100,          # number of warmup iterations per chain
#   iter_sampling = 100
# )
# fit_Bayes_SMEP$print(c("beta_mu","beta_sigma","phi_mu","phi_sigma","persev_mu","persev_sigma"))
# fit_Bayes_SMEP$print(c("beta_diff","phi_diff","perserv_diff"))
# stanfit <- rstan::read_stan_csv(fit_Bayes_SMEP$output_files())


# running MCMC
fit_Bayes_SMEP_cond <- stan(
  file = "Bayes_SMEP_cond.stan",  # Stan program
  data = bayes_data_list_1,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 200,          # number of warmup iterations per chain
  iter = 700,            # total number of iterations per chain
  cores = 4
  )

traceplot(fit_Bayes_SMEP_cond, pars = c("beta_mu","beta_sigma","phi_mu","phi_sigma","persev_mu","persev_sigma"), inc_warmup=T)
print(fit_Bayes_SMEP_cond, pars = c("beta_mu","beta_sigma","phi_mu","phi_sigma","persev_mu","persev_sigma", 
  "beta_diff","phi_diff","persev_diff"))
stan_hist(fit_Bayes_SMEP_cond, pars = c("beta_mu","beta_sigma","phi_mu","phi_sigma","persev_mu","persev_sigma", 
  "beta_diff","phi_diff","persev_diff"))
loo_fit_Bayes_SMEP_cond <- loo::loo(fit_Bayes_SMEP_cond)

# plot the parameters for difference between conditions
plot(fit_Bayes_SMEP_cond,show_density = TRUE, pars = "beta_diff")
plot(fit_Bayes_SMEP_cond,show_density = TRUE, pars = "phi_diff")
plot(fit_Bayes_SMEP_cond,show_density = TRUE, pars = "persev_diff")

```

There seems to be both a significant difference in beta and persev hyperparameters, where people in the imperative condition tended to place more weight on expected value when making bandit choices (beta), and those in the interrogative condition valued tended to repeat their previous choice (perseveration). However, compared to the values from Chakroun et al., these beta_mu's are really low, persev_mu rather large.

We can try this model again with somewhat less informative priors:

```{r}
# running MCMC
fit_Bayes_SMEP_cond_2 <- stan(
  file = "Bayes_SMEP_cond.stan",  # Stan program
  data = bayes_data_list_1,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 200,          # number of warmup iterations per chain
  iter = 700,            # total number of iterations per chain
  cores = 4
  )

traceplot(fit_Bayes_SMEP_cond_2, pars = c("beta_mu","beta_sigma","phi_mu","phi_sigma","persev_mu","persev_sigma"), inc_warmup=T)
print(fit_Bayes_SMEP_cond_2, pars = c("beta_mu","beta_sigma","phi_mu","phi_sigma","persev_mu","persev_sigma","beta_diff","phi_diff", "persev_diff"), inc_warmup=T)
```

Different results with different priors...

But, we can try to classify exploit, direct explore, and random explore choices based on the the model-produced Q values and the exploration bonuses, and see if there are significant group differences between the results from different priors.

