---
title: "Prolific_data_analysis.rmd"
author: "Candice Wang"
date: "3/22/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## load choice data

```{r}
library(dplyr)
library(ggplot2)
learn_data <- read.csv("/Volumes/main2/studies/Context_RL/Data/context_RL_learn.csv")

# obtain reward list info based on reward sequence
learn_data$reward_list <- NA
for (sub in 1:length(unique(learn_data$ID))){
  if(as.numeric(as.factor(unique(learn_data$ID))[sub]) < 39){ # only 92 trials for this subject
      start_ind <- 100*(as.numeric(as.factor(unique(learn_data$ID))[sub])-1)+1
      end_ind <- 100*as.numeric(as.factor(unique(learn_data$ID))[sub])
  }else if(as.numeric(as.factor(unique(learn_data$ID))[sub]) == 39){
      start_ind <- 100*(as.numeric(as.factor(unique(learn_data$ID))[sub])-1)+1
      end_ind <- 100*as.numeric(as.factor(unique(learn_data$ID))[sub]) - 8
  }else{
      start_ind <- 100*(as.numeric(as.factor(unique(learn_data$ID))[sub])-1)+1 -8
      end_ind <- 100*as.numeric(as.factor(unique(learn_data$ID))[sub]) -8
  }
  if(learn_data$blue_val[learn_data$learn_trial_n==1][as.numeric(as.factor(unique(learn_data$ID))[sub])] == 37.634){
    learn_data$reward_list[start_ind:end_ind] <- 3
  }else if(learn_data$blue_val[learn_data$learn_trial_n==1][as.numeric(as.factor(unique(learn_data$ID))[sub])] == 39.894){
    learn_data$reward_list[start_ind:end_ind] <- 1
  }else{
    learn_data$reward_list[start_ind:end_ind] <- 2
  }
}

#convert door choice into numeric variable
### door numbers
door_key <- c("red","blue","purple","yellow")
learn_data <- learn_data %>%
    mutate(chosen_door_num = case_when(
      grepl(door_key[1],chosen_door) ~ 1,
      grepl(door_key[2],chosen_door) ~ 2,
      grepl(door_key[3],chosen_door) ~ 3,
      grepl(door_key[4],chosen_door) ~ 4
    ))
learn_data$ID_num <- as.numeric(as.factor(learn_data$ID))
```

## model-based analysis of choice data

We can try fitting the model with a delta learning rule (same learning rate on every trial for a given subject) or bayesian learning rule (adaptive learning rate based on estimate uncertainty) for reward updating. For the choice rule, we can model a simple softmax choice rule with an inverse temperature parameter that governs how much a participant relies on estimated values of the options when making choices, and also add in additional parameters such as directed exploration bonus (how much a participant makes choices to resolve uncertainty about the reward estimate of options) and perserveration bonus (how much a participant tends to repeat their choice on the last trial).

We expect that participants in the interrogative condition would have greater directed exploration compared to participants in the imperative condition, while the other parameters would not necessarily differ. We can model them as two separate populations and estimate the *difference* in directed exploration, random exploration, and perserveration free parameters between the two groups.

```{r}
library(rstan)
rstan_options(javascript=FALSE)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
# compute exploration bonus for delta learning rule: scales linearly with the time it's been since an option was last chosen
exploration_bonus <- data.frame(matrix(nrow = nrow(learn_data), ncol = 4))
for (t in 1:(nrow(learn_data)-1)){
  if(learn_data$learn_trial_n[t]==1){
    exploration_bonus[t,] <- 1
  }
  for (i in 1:4){
     # for each arm, see check the last time that it was chosen
      if(length(which(learn_data$chosen_door_num[(1+(learn_data$ID_num[t]-1)*100):t] %in% i))==0){
        exploration_bonus[t+1,i] <- t - (learn_data$ID_num[t]-1)*100 # hasn't been chosen before
      }else{
        exploration_bonus[t+1,i] <- t - (max(which(learn_data$chosen_door_num[(1+(learn_data$ID_num[t]-1)*100):t] %in% i)) + (learn_data$ID_num[t]-1)*100)
      }
  }
}

# make data lists for stan
delta_data_list  <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward,
             eb = exploration_bonus
             ) 
bayes_data_list <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward
)

# with condition information
learn_data_list_delta <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward,
             eb = exploration_bonus, 
             condition = as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1]))==1)
             ) 
bayes_data_list_1 <- list(
             totalTrials = nrow(learn_data), 
             nSubjects = length(unique(learn_data$ID)), 
             subject = learn_data$ID_num, 
             trialNum = learn_data$learn_trial_n,
             choices = learn_data$chosen_door_num, 
             rewards = learn_data$reward,
             condition = as.numeric(as.factor(learn_data$condition[learn_data$learn_trial_n==1]))
)
```


## model comparison

6 models were fit to the choice data on Cerberus (Bayesian learning rule or delta learning rule, estimated value only choice rule or with the addition of directed exploration bonus or both directed exploration and perserveration bonus). We can use leave-one-out cross validation estimates to compare the predictive accuracies of different models.

```{r}
library(loo)
loo_delta_SM <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/loo_learn_fit_delta_SM.rds")
loo_delta_SME <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/loo_learn_fit_delta_SME.rds")
loo_delta_SMEP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/loo_learn_fit_delta_SMEP.rds")
loo_bayes_SM <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/loo_learn_fit_bayes_SM.rds")
loo_bayes_SME <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/loo_learn_fit_bayes_SME.rds")
loo_bayes_SMEP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/loo_learn_fit_bayes_SMEP.rds")

# use the loo_compare function to compare multiple models on expected log predictive density (ELPD) for new data:
loo_compare(loo_delta_SM, loo_delta_SME, loo_delta_SMEP, loo_bayes_SM, loo_bayes_SME, loo_bayes_SMEP)

log_likelihoods <- data.frame(matrix(nrow = 6, ncol = 2))
colnames(log_likelihoods) <- c("model_name","elpd")
log_likelihoods$model_name[1] <- "delta-SM"
log_likelihoods$elpd[1] <- loo_delta_SM$estimates[1]
log_likelihoods$model_name[2] <- "delta-SME"
log_likelihoods$elpd[2] <- loo_delta_SME$estimates[1]
log_likelihoods$model_name[3] <- "delta-SMEP"
log_likelihoods$elpd[3] <- loo_delta_SMEP$estimates[1]
log_likelihoods$model_name[4] <- "bayes-SM"
log_likelihoods$elpd[4] <- loo_bayes_SM$estimates[1]
log_likelihoods$model_name[5] <- "bayes-SME"
log_likelihoods$elpd[5] <- loo_bayes_SME$estimates[1]
log_likelihoods$model_name[6] <- "bayes-SMEP"
log_likelihoods$elpd[6] <- loo_bayes_SMEP$estimates[1]
# divide by the total number of data points in the sample (n*t)
log_likelihoods <- log_likelihoods %>%
  mutate(elpd_div_trial = elpd/nrow(learn_data))

# plot
levels_order <- c("delta-SM","bayes-SM","delta-SME","bayes-SME","delta-SMEP","bayes-SMEP")

log_likelihoods %>%
  ggplot(aes(x = factor(model_name, level = levels_order), y = elpd_div_trial))+
  geom_bar(stat="identity")+
  coord_cartesian(ylim = c(-1.1, -0.8))+
  ylab("loo log-likelihood")+
  xlab("")+
  theme_allie()


```

The Bayesian learning model with terms for directed exploration and perseveration (bayes-SMEP) showed highest predictive accuracy. This is the winning model with which the following analyses are conducted.

## parameter recovery on the winning model

We can simulate a range of values for the group difference hyperparameters of interest (beta_mu_diff and phi_mu_diff) and see if the model is able to recover the true parameters.

```{r}
# from bandit_sim.rmd
# the 3rd value is the estimated value from observed data
beta_mu_diff_sim <- c(0, 0.01, 0.02, 0.03, 0.04)
phi_mu_diff_sim <- c(-1, -0.75, -0.5, -0.25, 0)
# the rest of the hyperparameters were set to the values estimated from observed data

# from fitted objects
beta_1 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_beta_1_params.rds")
beta_2 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_beta_2_params.rds")
beta_phi_3 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_beta_phi_persev_3_params.rds")
beta_4 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_beta_4_params.rds")
beta_5 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_beta_5_params.rds")
beta_mu_diff_fit <- c(mean(beta_1$beta_mu_diff), mean(beta_2$beta_mu_diff), mean(beta_phi_3$beta_mu_diff),
                      mean(beta_4$beta_mu_diff), mean(beta_5$beta_mu_diff))

phi_1 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_phi_1_params.rds")
phi_2 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_phi_2_params.rds")
phi_4 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_phi_4_params.rds")
phi_5 <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/param_recov/sim_data_fit_phi_5_params.rds")
phi_mu_diff_fit <- c(mean(phi_1$phi_mu_diff), mean(phi_2$phi_mu_diff), mean(beta_phi_3$phi_mu_diff),
                      mean(phi_4$phi_mu_diff), mean(phi_5$phi_mu_diff))

# correlation
agreement_betas <- cor(beta_mu_diff_sim, beta_mu_diff_fit)
plot(beta_mu_diff_sim, beta_mu_diff_fit, main="Simulated vs. fitted group difference in inverse temperature", xlab="simulated", ylab="fitted")
print(agreement_betas)

agreement_phis <- cor(phi_mu_diff_sim, phi_mu_diff_fit)
plot(phi_mu_diff_sim, phi_mu_diff_fit, main="Simulated vs. fitted group differences in directed exploration", xlab="simulated", ylab="fitted")
print(agreement_phis)
```

The fitted group difference hyperparameter values are highly correlated with the simulated (true) values. There seems to be a tendency to under-estimate high values of beta (inverse temperature) difference, but this should not be an issue for our hypothesis.

## group differences 

We can look at group differences in inverse temperature (beta), directed exploration (phi), and perserveration (persev) by examining the corresponding hyperparameters.

```{r}
bayes_SMEP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/fit_obj/bayes_SMEP_samples_selectparams.rds")
library(ggdist)
# make dataframes for hyperparamters
beta_mu_diff <- as.data.frame(bayes_SMEP$beta_mu_diff)
colnames(beta_mu_diff) <- "beta_mu_diff_draws"
phi_mu_diff <- as.data.frame(bayes_SMEP$phi_mu_diff)
colnames(phi_mu_diff) <- "phi_mu_diff_draws"
persev_mu_diff <- as.data.frame(bayes_SMEP$persev_mu_diff)
colnames(persev_mu_diff) <- "persev_mu_diff_draws"

# make plots
beta_mu_diff %>%
  ggplot(aes(x = beta_mu_diff_draws)) +
  # point estimate = mean; high density continuous interval at 95%
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+ 
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlab("hyperparameter posterior for group difference in inverse temperature")+
  ylab("")+
  theme_allie()

phi_mu_diff %>%
  ggplot(aes(x = phi_mu_diff_draws)) +
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlab("hyperparameter posterior for group difference in directed exploration")+
  ylab("")+
  theme_allie()

persev_mu_diff %>%
  ggplot(aes(x = persev_mu_diff_draws)) +
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlab("hyperparameter posterior for group difference in perserveration")+
  ylab("")+
  theme_allie()

```

Participants in the imperative condition made choices based on estimated values of the options to a greater extent compared to those in the interrogative condition, while participants in the interrogative condition tended to make choices to resolve uncertainty to a greater extent than those in the imperative condition. The two groups did not significantly differ in their tendency to repeat their previous choice (95% CI of the posterior spans 0).

# model-derived choice classification

We can classify whether a trial is exploitative or exploratory based on the model-estiamted Q value, and whether it is directed exploration (choosing the choice with the most uncertainty.highest directed exploration bonus) or random exploration (one of the other choices). We can then compare the likelihood of making a choice that falls under these three categories across the two conditions, imperative and interrogative.

```{r}
# model-estimated Q value 
bayes_SMEP_Q <- as.data.frame(apply(bayes_SMEP$v, c(2,3), mean))
bayes_SMEP_Q <- bayes_SMEP_Q %>%
  cbind(max_Q_choice = max.col(bayes_SMEP_Q, 'first'))

# model-estimated directed exploration bonus 
bayes_SMEP_eb <- as.data.frame(apply(bayes_SMEP$eb, c(2,3), mean))
colnames(bayes_SMEP_eb) <- c("eb_1","eb_2","eb_3","eb_4")
bayes_SMEP_eb <- bayes_SMEP_eb %>%
  cbind(max_eb_choice = max.col(bayes_SMEP_eb, 'first'))

bayes_SMEP_eb$learn_trial_n <- learn_data$learn_trial_n
bayes_SMEP_eb$subject <- learn_data$ID_num
bayes_SMEP_eb$condition <- as.factor(learn_data$condition)
bayes_SMEP_eb$reward_list <- as.factor(learn_data$reward_list)
  
# classify choice on each trial 
choice_classification <- bayes_SMEP_eb %>%
  cbind(choice = learn_data$chosen_door_num) %>%
  cbind(bayes_SMEP_Q) %>%
  mutate(choice_type = case_when(
    choice == max_Q_choice ~ "exploitation",
    choice == max_eb_choice ~ "directed_exploration",
    #TRUE ~ "a_random_exploration" # to make this the default ref group for multinomial logit regression
    TRUE ~ "random_exploration"
  ),choice_type = as.factor(choice_type))

# merge with learn data
learn_data_choice_classification <- merge(learn_data, choice_classification, by.x = c("ID_num","learn_trial_n"), by.y = c("subject","learn_trial_n"))

# plot
levels_order <- c("exploitation","directed_exploration","random_exploration")
choice_classification %>%
  group_by(subject, condition) %>%
  count(choice_type) %>%
  ungroup() %>%
  group_by(condition) %>%
  ggplot(aes(x = factor(choice_type, level = levels_order), y = n, color = condition))+
  geom_boxplot()+
  stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="pointrange", position = position_dodge(0.75))+
  theme_allie()+
  scale_color_red_blue("Information Seeking Condition")+
  xlab("")+
  ylab("number of choices (out of 100 trials)")+
  scale_x_discrete(labels=c("exploitation" = "exploitation", "directed_exploration" = "directed exploration",
                              "random_exploration" = "random exploration"))

# some stats
library(mclogit)
mod <- mblogit(choice_type ~ condition + reward_list, data = choice_classification, random = ~1|subject)
summary(mod)

# pairwise comparisons
choice_summary <- choice_classification %>%
  group_by(subject,condition) %>%
  count(choice_type)

imp_exploit <- filter(choice_summary, choice_type == "exploitation") %>% filter(condition == "imperative")
int_exploit <- filter(choice_summary, choice_type == "exploitation") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(int_exploit$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_exploit$n, int_exploit$n, alternative = "two.sided")

imp_direxp <- filter(choice_summary, choice_type == "directed_exploration") %>% filter(condition == "imperative")
int_direxp <- filter(choice_summary, choice_type == "directed_exploration") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(int_direxp$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_direxp$n, int_direxp$n, alternative = "two.sided")

imp_randexp <- filter(choice_summary, choice_type == "random_exploration") %>% filter(condition == "imperative")
int_randexp <- filter(choice_summary, choice_type == "random_exploration") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(int_randexp$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_randexp$n, int_randexp$n, alternative = "two.sided")
```

Based on the the multinomial logistic regression, the interrogative group made more directed exploration choices to resolve uncertainty compared to the imperative group, and the imperative group made more exploitation choices to maximize points gained.

## individual parameter estimates

We can also get the individual parameter esimates for inverse temperature, directed exploration, and perserveration. 

```{r}
# get the point estimate (mean) for individual subject level parameters
betas <- apply(bayes_SMEP$beta, 2, mean)
phis <- apply(bayes_SMEP$phi, 2, mean)
persevs <- apply(bayes_SMEP$persev, 2, mean)
```

Learning rate (Kgain) and prediction error (pe) for each trial:

```{r}
# learning rate on each trial
Kgains <- apply(bayes_SMEP$Kgain, 2, mean)

# combine with learn data
for(row in 1:nrow(learn_data)){
  learn_data$inverse_temperature[row] <- betas[learn_data$ID_num[row]]
  learn_data$directed_exploration[row] <- phis[learn_data$ID_num[row]]
  learn_data$perseveration[row] <- persevs[learn_data$ID_num[row]]
  learn_data$learning_rate[row] <- Kgains[row]
}

# prediction error on each trial is received - expected reward for the chosen door 
for(row in 1:nrow(learn_data)){
  learn_data$prediction_error[row] <- learn_data$reward[row] - bayes_SMEP_Q[row, learn_data$chosen_door_num[row]]
}

# export 
write.csv(learn_data, file = "/Users/CandiceWang/Documents/CNAP/Projects/contextRL/learn_data_params.csv")
```


