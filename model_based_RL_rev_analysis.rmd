---
title: "model_based_RL_rev_analysis"
author: "Candice Wang"
date: "2023-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Combine participants from S1 and S2 into one sample (N=200) since they were run on the same platform with the exact task.

```{r load_data}
library(dplyr)
library(ggplot2)
# read in S1 data 
learn_data_S1 <- read.csv("/Volumes/main2/studies/Context_RL/Data/context_RL_learn.csv")
# add reward_list to S1 data
learn_data_S1$reward_list <- NA
for (sub in 1:length(unique(learn_data_S1$ID))){
  if(sub < 39){ # only 92 trials for this subject
      start_ind <- 100*(sub-1)+1
      end_ind <- 100*sub
  }else if(sub == 39){
      start_ind <- 100*(sub-1)+1
      end_ind <- 100*sub - 8
  }else{
      start_ind <- 100*(sub-1)+1 -8
      end_ind <- 100*sub -8
  }
  
  if(learn_data_S1$blue_val[learn_data_S1$learn_trial_n==1][sub] == 37.634){
    learn_data_S1$reward_list[start_ind:end_ind] <- 3
  }else if(learn_data_S1$blue_val[learn_data_S1$learn_trial_n==1][sub] == 39.894){
    learn_data_S1$reward_list[start_ind:end_ind] <- 1
  }else{
    learn_data_S1$reward_list[start_ind:end_ind] <- 2
  }
}
# numerical subject ID and sample info
learn_data_S1$ID_num <- as.numeric(as.factor(learn_data_S1$ID))
learn_data_S1$sample <- 1
# read in S2 data
learn_data_S2 <- read.csv("/Volumes/main2/studies/Context_RL/Data/context_RL_learn_replication.csv")
# add session to S2 data and add numeric subject numbers
learn_data_S2$ID_num <- as.numeric(as.factor(learn_data_S2$ID))+100
learn_data_S2$sample <- 2
# remove extra var in S2
learn_data_S2 <- learn_data_S2[-c(7)]
# combine S1+S2 
learn_df <- rbind(learn_data_S1, learn_data_S2)
#convert door choice into numeric variable
### door numbers
door_key <- c("red","blue","purple","yellow")
learn_df <- learn_df %>%
    mutate(chosen_door_num = case_when(
      grepl(door_key[1],chosen_door) ~ 1,
      grepl(door_key[2],chosen_door) ~ 2,
      grepl(door_key[3],chosen_door) ~ 3,
      grepl(door_key[4],chosen_door) ~ 4
    ))

# create lists for modeling
bayes_data_list <- list(
             totalTrials = nrow(learn_df), 
             nSubjects = length(unique(learn_df$ID)), 
             subject = learn_df$ID_num, 
             trialNum = learn_df$learn_trial_n,
             choices = learn_df$chosen_door_num, 
             rewards = learn_df$reward,
             condition = as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1]))
)
delta_data_list <- list(
             totalTrials = nrow(learn_df), 
             nSubjects = length(unique(learn_df$ID)), 
             subject = learn_df$ID_num, 
             trialNum = learn_df$learn_trial_n,
             choices = learn_df$chosen_door_num, 
             rewards = learn_df$reward,
             condition = as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1]))==1)
)
# compute exploration bonus for delta learning rule: scales linearly with the time it's been since an option was last chosen
exploration_bonus <- data.frame(matrix(nrow = nrow(learn_df), ncol = 4))
for (t in 1:(nrow(learn_df)-1)){
  if(learn_df$learn_trial_n[t]==1){
    exploration_bonus[t,] <- 1
  }
  for (i in 1:4){
    # check if this is the sub with 8 missing trials
    if(learn_df$ID_num[t] <= 39){
      # for each arm, see check the last time that it was chosen
      if(length(which(learn_df$chosen_door_num[(1+(learn_df$ID_num[t]-1)*100):t] %in% i))==0){
        exploration_bonus[t+1,i] <- t - (learn_df$ID_num[t]-1)*100 # hasn't been chosen before
      }else{
        exploration_bonus[t+1,i] <- t - (max(which(learn_df$chosen_door_num[(1+(learn_df$ID_num[t]-1)*100):t] %in% i)) + (learn_df$ID_num[t]-1)*100)
      }
    }else{
      # for each arm, see check the last time that it was chosen
      if(length(which(learn_df$chosen_door_num[(1+(learn_df$ID_num[t]-1)*100 - 8):t] %in% i))==0){
        exploration_bonus[t+1,i] <- t - (learn_df$ID_num[t]-1)*100 + 8 # hasn't been chosen before
      }else{
        exploration_bonus[t+1,i] <- t - (max(which(learn_df$chosen_door_num[(1+(learn_df$ID_num[t]-2)*100+92):t] %in% i)) + (learn_df$ID_num[t]-2)*100+92)
      }
    }
     
  }
}
delta_data_list_eb <- list(
             totalTrials = nrow(learn_df), 
             nSubjects = length(unique(learn_df$ID)), 
             subject = learn_df$ID_num, 
             trialNum = learn_df$learn_trial_n,
             choices = learn_df$chosen_door_num, 
             rewards = learn_df$reward,
             eb = exploration_bonus,
             condition = as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1]))==1)
)
### just ssample 1 (for re-doing deltaSMEP fitting)
delta_data_list_eb_sample1 <- list(
  totalTrials = nrow(learn_df_s1), 
             nSubjects = length(unique(learn_df_s1$ID)), 
             subject = learn_df_s1$ID_num, 
             trialNum = learn_df_s1$learn_trial_n,
             choices = learn_df_s1$chosen_door_num, 
             rewards = learn_df_s1$reward,
             eb = exploration_bonus[1:9992,],
             condition = as.numeric(as.factor(learn_df_s1$condition[learn_df_s1$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_df_s1$condition[learn_df_s1$learn_trial_n==1]))==1)
)
```

model-free measures of behavioral performance

```{r model_free}
# total reward
obs_dat <- learn_df %>%
  mutate(trial_bins = cut(learn_trial_n, breaks = 5))%>%
  mutate(trial_bins = as.numeric(as.factor(trial_bins)))%>%
  group_by(ID_num, trial_bins) %>%
  summarise(total_reward = sum(reward)) %>%
  ggplot(aes(x = as.factor(trial_bins), y = total_reward))+
  geom_boxplot()+
  geom_jitter(alpha = 0.5)+
  xlab("trial bins")+
  #facet_wrap("reward_list")+
  ggtitle("Observed data")+
  theme_allie()

# p_optimal
learn_df %>%
  mutate(trial_bins = cut(learn_trial_n, breaks = 5))%>%
  mutate(trial_bins = as.numeric(as.factor(trial_bins)))%>%
  group_by(ID_num, trial_bins, reward_list) %>%
  summarise(p_optimal = sum(optimal_choice)/n()) %>%
  ggplot(aes(x = as.factor(trial_bins), y = p_optimal))+
  geom_boxplot()+
  geom_jitter(alpha = 0.5)+
  facet_wrap("reward_list")
```


model comparison

```{r}
library(loo)
loo_delta_SM <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_delta_SM_loo.rds")
loo_delta_SMP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_delta_SMP_loo.rds")
loo_delta_SME <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_delta_SME_loo.rds")
loo_delta_SMEP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_delta_SMEP_loo.rds")
loo_bayes_SM <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_bayes_SM_loo.rds")
loo_bayes_SMP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_bayes_SMP_loo.rds")
loo_bayes_SME <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_bayes_SME_loo.rds")
loo_bayes_SMEP <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_bayes_SMEP_loo.rds")

# use the loo_compare function to compare multiple models on expected log predictive density (ELPD) for new data:
loo_compare(loo_delta_SM, loo_delta_SMP, loo_delta_SME, loo_delta_SMEP, loo_bayes_SM, loo_bayes_SMP, loo_bayes_SME, loo_bayes_SMEP)

log_likelihoods <- data.frame(matrix(nrow = 8, ncol = 2))
colnames(log_likelihoods) <- c("model_name","elpd")
log_likelihoods$model_name[1] <- "delta\nSM"
log_likelihoods$elpd[1] <- loo_delta_SM$estimates[1]
log_likelihoods$model_name[2] <- "delta\nSME"
log_likelihoods$elpd[2] <- loo_delta_SME$estimates[1]
log_likelihoods$model_name[3] <- "delta\nSMP"
log_likelihoods$elpd[3] <- loo_delta_SMP$estimates[1]
log_likelihoods$model_name[4] <- "delta\nSMEP"
log_likelihoods$elpd[4] <- loo_delta_SMEP$estimates[1]
log_likelihoods$model_name[5] <- "bayes\nSM"
log_likelihoods$elpd[5] <- loo_bayes_SM$estimates[1]
log_likelihoods$model_name[6] <- "bayes\nSME"
log_likelihoods$elpd[6] <- loo_bayes_SME$estimates[1]
log_likelihoods$model_name[7] <- "bayes\nSMP"
log_likelihoods$elpd[7] <- loo_bayes_SMP$estimates[1]
log_likelihoods$model_name[8] <- "bayes\nSMEP"
log_likelihoods$elpd[8] <- loo_bayes_SMEP$estimates[1]
# divide by the total number of data points in the sample (n*t)
log_likelihoods <- log_likelihoods %>%
  mutate(elpd_div_trial = elpd/nrow(learn_df))

# plot
levels_order <- c("delta\nSM","bayes\nSM","delta\nSMP","bayes\nSMP","delta\nSME","bayes\nSME","delta\nSMEP","bayes\nSMEP")

log_likelihoods %>%
  ggplot(aes(x = factor(model_name, level = levels_order), y = elpd_div_trial))+
  geom_bar(stat="identity")+
  coord_cartesian(ylim = c(-1.1, -0.8))+
  ylab("loo log-likelihood")+
  xlab("")+
  theme_allie()

```
delta_SMEP is the winning model

model-fitting on the Cerberus (rev_analysis/combined_sample_learn_fit_delta_SMEP.r)

```{r group_diff_params}
# posterior draws extracted from fitted object
delta_SMEP_draws <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_delta_SMEP_draws.rds")

library(ggdist)
# make dataframes for hyperparamters
alpha_mu_diff <- as.data.frame(delta_SMEP_draws$alpha_mu_diff)
colnames(alpha_mu_diff) <- "alpha_mu_diff_draws"
beta_mu_diff <- as.data.frame(delta_SMEP_draws$beta_mu_diff)
colnames(beta_mu_diff) <- "beta_mu_diff_draws"
phi_mu_diff <- as.data.frame(delta_SMEP_draws$phi_mu_diff)
colnames(phi_mu_diff) <- "phi_mu_diff_draws"
persev_mu_diff <- as.data.frame(delta_SMEP_draws$persev_mu_diff)
colnames(persev_mu_diff) <- "persev_mu_diff_draws"

library(tidybayes)
tidybayes::hdci(alpha_mu_diff$alpha_mu_diff_draws, .width = 0.95)
tidybayes::hdci(beta_mu_diff$beta_mu_diff_draws, .width = 0.95)
tidybayes::hdci(phi_mu_diff$phi_mu_diff_draws, .width = 0.95)
tidybayes::hdci(persev_mu_diff$persev_mu_diff_draws, .width = 0.95)

library(ggplot2)
# make plots
alpha_mu_diff %>%
  ggplot(aes(x = alpha_mu_diff_draws)) +
  # point estimate = mean; quantile interval at 95%
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+ 
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  # xlim(c(-0.06,0.06))+
  xlab("learning rate")+
  ylab("")+
  theme_allie()

beta_mu_diff %>%
  ggplot(aes(x = beta_mu_diff_draws)) +
  # point estimate = mean; quantile interval at 95%
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+ 
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlim(c(-0.06,0.06))+
  xlab("inverse temperature")+
  ylab("")+
  theme_allie()

phi_mu_diff %>%
  ggplot(aes(x = phi_mu_diff_draws)) +
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlim(c(-0.6,0.6))+
  xlab("directed exploration")+
  ylab("")+
  theme_allie()

persev_mu_diff %>%
  ggplot(aes(x = persev_mu_diff_draws)) +
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlab("hyperparameter posterior for group difference in perserveration")+
  ylab("")+
  theme_allie()
```

we can classify choices based on model-estimated expected values and estimated uncertainties

```{r}
# model-estimated Q value 
delta_SMEP_Q <- as.data.frame(apply(delta_SMEP_draws$Q, c(2,3), mean))
delta_SMEP_Q <- delta_SMEP_Q %>%
  cbind(max_Q_choice = max.col(delta_SMEP_Q, 'first'))

# directed exploration bonus 
colnames(exploration_bonus) <- c("eb_1","eb_2","eb_3","eb_4")
exploration_bonus <- exploration_bonus %>%
  cbind(max_eb_choice = max.col(exploration_bonus, 'first'))

exploration_bonus$learn_trial_n <- learn_df$learn_trial_n
exploration_bonus$subject <- learn_df$ID_num
exploration_bonus$condition <- as.factor(learn_df$condition)
exploration_bonus$reward_list <- as.factor(learn_df$reward_list)
  
# classify choice on each trial 
choice_classification <- exploration_bonus %>%
  cbind(choice = learn_df$chosen_door_num) %>%
  cbind(delta_SMEP_Q) %>%
  mutate(choice_type = case_when(
    choice == max_Q_choice ~ "exploitation",
    choice == max_eb_choice ~ "directed_exploration",
    #TRUE ~ "a_random_exploration" # to make this the default ref group for multinomial logit regression
    TRUE ~ "random_exploration"
  ),choice_type = as.factor(choice_type))

# merge with learn data
learn_data_choice_classification <- merge(learn_df, choice_classification, by.x = c("ID_num","learn_trial_n"), by.y = c("subject","learn_trial_n"))

# plot
levels_order <- c("exploitation","directed_exploration","random_exploration")
choice_plot <- choice_classification %>%
  group_by(subject, condition) %>%
  count(choice_type) %>%
  ungroup() %>%
  group_by(condition) %>%
  ggplot(aes(x = factor(choice_type, level = levels_order), y = n, color = condition))+
  geom_boxplot()+
  stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="pointrange", position = position_dodge(0.75))+
  theme_allie()+
  scale_color_red_blue("Condition")+
  xlab("")+
  ylab("number of choices (out of 100 trials)")+
  scale_x_discrete(labels=c("exploitation" = "exploitation", "directed_exploration" = "directed exploration",
                              "random_exploration" = "random exploration"))+
  ggtitle("Number of Exploitation and Exploration\nChoices by Condition (Sample 1)")
choice_plot

# some stats
library(mclogit)
mod <- mblogit(choice_type ~ condition + reward_list, data = choice_classification, random = ~1|subject)
summary(mod)

# pairwise comparisons
choice_summary <- choice_classification %>%
  group_by(subject,condition) %>%
  count(choice_type)

imp_exploit <- filter(choice_summary, choice_type == "exploitation") %>% filter(condition == "imperative")
int_exploit <- filter(choice_summary, choice_type == "exploitation") %>% filter(condition == "interrogative")
# normal distribution?
shapiro.test(imp_exploit$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_exploit$n, int_exploit$n, alternative = "two.sided")

imp_direxp <- filter(choice_summary, choice_type == "directed_exploration") %>% filter(condition == "imperative")
int_direxp <- filter(choice_summary, choice_type == "directed_exploration") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(int_direxp$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_direxp$n, int_direxp$n, alternative = "two.sided")

imp_randexp <- filter(choice_summary, choice_type == "random_exploration") %>% filter(condition == "imperative")
int_randexp <- filter(choice_summary, choice_type == "random_exploration") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(imp_randexp$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_randexp$n, int_randexp$n, alternative = "two.sided")

## effect sizes
# choice_summary %>%
#   filter(choice_type == "exploitation") %>%
#   ungroup() %>%
#   wilcox_effsize(n ~ condition)

effectsize::cohens_d(n~condition, data = choice_summary %>%
  filter(choice_type == "exploitation") %>%
  ungroup())

effectsize::cohens_d(n~condition, data = choice_summary %>%
  filter(choice_type == "directed_exploration") %>%
  ungroup(), pooled_sd = F)

effectsize::cohens_d(n~condition, data = choice_summary %>%
  filter(choice_type == "random_exploration") %>%
  ungroup())
```

plot the trial-by-trial model-derived expected value and estimated uncertainty for an example subject 

```{r}
# door colors: red blue purple yellow
door_colors = c("#BD431E", "#3772C6", "#8368C7", "#F6C34C")

#  example subject choice classification
sub_num = 15 # interrogative

# plot trial by trial expected value
learn_data_choice_classification %>%
  filter(ID_num==sub_num)%>%
  ggplot(aes(x = learn_trial_n))+
  geom_line(aes(y = V1), color = door_colors[1])+
  geom_line(aes(y = V2), color = door_colors[2])+
  geom_line(aes(y = V3), color = door_colors[3])+
  geom_line(aes(y = V4), color = door_colors[4])+
  theme_allie()+
  xlab("Trial #")+
  ylab("Expected Value")+
  ggtitle("Model-Derived Expected Value (example interrogative participant)")+
  ylim(0,100)

# plot trial by trial estimated uncertainty
learn_data_choice_classification %>%
  filter(ID_num==sub_num)%>%
  ggplot(aes(x = learn_trial_n))+
  geom_line(aes(y = eb_1), color = door_colors[1])+
  geom_line(aes(y = eb_2), color = door_colors[2])+
  geom_line(aes(y = eb_3), color = door_colors[3])+
  geom_line(aes(y = eb_4), color = door_colors[4])+
  theme_allie()+
  xlab("Trial #")+
  ylab("Uncertainty")+
  ggtitle("Uncertainty (example interrogative participant)")+
  ylim(0,45)
```

individual level params

```{r params_extract}
# get the point estimate (mean) for individual subject level parameters
alphas <- apply(delta_SMEP_draws$alpha, 2, mean)
betas <- apply(delta_SMEP_draws$beta, 2, mean)
phis <- apply(delta_SMEP_draws$phi, 2, mean)
persevs <- apply(delta_SMEP_draws$persev, 2, mean)

# combine with learn data
for(row in 1:nrow(learn_df)){
  learn_df$inverse_temperature[row] <- betas[learn_df$ID_num[row]]
  learn_df$directed_exploration[row] <- phis[learn_df$ID_num[row]]
  learn_df$perseveration[row] <- persevs[learn_df$ID_num[row]]
  learn_df$learning_rate[row] <- alphas[learn_df$ID_num[row]]
}

# prediction error on each trial is received - expected reward for the chosen door 
for(row in 1:nrow(learn_df)){
  learn_df$prediction_error[row] <- learn_df$reward[row] - delta_SMEP_Q[row, learn_df$chosen_door_num[row]]
}

# append choice type
learn_df$choice_type <- learn_data_choice_classification$choice_type

# export 
write.csv(learn_df, file = "/Users/candiceyuxiwang/Documents/CNAP/Projects/contextRL/combined_sample_learn_data_params.csv")
```

