---
title: "model_based_RL_rev_analysis"
author: "Candice Wang"
date: "2023-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Combine participants from S1 and S2 into one sample (N=200) since they were run on the same platform with the exact task.

```{r load_data}
library(dplyr)
library(ggplot2)
# read in S1 data 
learn_data_S1 <- read.csv("/Volumes/main2/studies/Context_RL/Data/context_RL_learn.csv")
# add reward_list to S1 data
learn_data_S1$reward_list <- NA
for (sub in 1:length(unique(learn_data_S1$ID))){
  if(sub < 39){ # only 92 trials for this subject
      start_ind <- 100*(sub-1)+1
      end_ind <- 100*sub
  }else if(sub == 39){
      start_ind <- 100*(sub-1)+1
      end_ind <- 100*sub - 8
  }else{
      start_ind <- 100*(sub-1)+1 -8
      end_ind <- 100*sub -8
  }
  
  if(learn_data_S1$blue_val[learn_data_S1$learn_trial_n==1][sub] == 37.634){
    learn_data_S1$reward_list[start_ind:end_ind] <- 3
  }else if(learn_data_S1$blue_val[learn_data_S1$learn_trial_n==1][sub] == 39.894){
    learn_data_S1$reward_list[start_ind:end_ind] <- 1
  }else{
    learn_data_S1$reward_list[start_ind:end_ind] <- 2
  }
}
# numerical subject ID and sample info
learn_data_S1$ID_num <- as.numeric(as.factor(learn_data_S1$ID))
learn_data_S1$sample <- 1
# read in S2 data
learn_data_S2 <- read.csv("/Volumes/main2/studies/Context_RL/Data/context_RL_learn_replication.csv")
# add session to S2 data and add numeric subject numbers
learn_data_S2$ID_num <- as.numeric(as.factor(learn_data_S2$ID))+100
learn_data_S2$sample <- 2
# remove extra var in S2
learn_data_S2 <- learn_data_S2[-c(7)]
# combine S1+S2 
learn_df <- rbind(learn_data_S1, learn_data_S2)
#convert door choice into numeric variable
### door numbers
door_key <- c("red","blue","purple","yellow")
learn_df <- learn_df %>%
    mutate(chosen_door_num = case_when(
      grepl(door_key[1],chosen_door) ~ 1,
      grepl(door_key[2],chosen_door) ~ 2,
      grepl(door_key[3],chosen_door) ~ 3,
      grepl(door_key[4],chosen_door) ~ 4
    ))

# create lists for modeling
bayes_data_list <- list(
             totalTrials = nrow(learn_df), 
             nSubjects = length(unique(learn_df$ID)), 
             subject = learn_df$ID_num, 
             trialNum = learn_df$learn_trial_n,
             choices = learn_df$chosen_door_num, 
             rewards = learn_df$reward,
             condition = as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1]))
)
delta_data_list <- list(
             totalTrials = nrow(learn_df), 
             nSubjects = length(unique(learn_df$ID)), 
             subject = learn_df$ID_num, 
             trialNum = learn_df$learn_trial_n,
             choices = learn_df$chosen_door_num, 
             rewards = learn_df$reward,
             condition = as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1]))==1)
)
# compute exploration bonus for delta learning rule: scales linearly with the time it's been since an option was last chosen
exploration_bonus <- data.frame(matrix(nrow = nrow(learn_df), ncol = 4))
for (t in 1:(nrow(learn_df)-1)){
  if(learn_df$learn_trial_n[t]==1){
    exploration_bonus[t,] <- 1
  }
  for (i in 1:4){
     # for each arm, see check the last time that it was chosen
      if(length(which(learn_df$chosen_door_num[(1+(learn_df$ID_num[t]-1)*100):t] %in% i))==0){
        exploration_bonus[t+1,i] <- t - (learn_df$ID_num[t]-1)*100 # hasn't been chosen before
      }else{
        exploration_bonus[t+1,i] <- t - (max(which(learn_df$chosen_door_num[(1+(learn_df$ID_num[t]-1)*100):t] %in% i)) + (learn_df$ID_num[t]-1)*100)
      }
  }
}
delta_data_list_eb <- list(
             totalTrials = nrow(learn_df), 
             nSubjects = length(unique(learn_df$ID)), 
             subject = learn_df$ID_num, 
             trialNum = learn_df$learn_trial_n,
             choices = learn_df$chosen_door_num, 
             rewards = learn_df$reward,
             eb = exploration_bonus,
             condition = as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1])),
             nSubCond1 = sum(as.numeric(as.factor(learn_df$condition[learn_df$learn_trial_n==1]))==1)
)
```

model-free measures of behavioral performance

```{r model_free}
# total reward
learn_df %>%
  mutate(trial_bins = cut(learn_trial_n, breaks = 5))%>%
  mutate(trial_bins = as.numeric(as.factor(trial_bins)))%>%
  group_by(ID_num, trial_bins, reward_list) %>%
  summarise(total_reward = sum(reward)) %>%
  ggplot(aes(x = as.factor(trial_bins), y = total_reward))+
  geom_boxplot()+
  geom_jitter(alpha = 0.5)+
  facet_wrap("reward_list")

# p_optimal
learn_df %>%
  mutate(trial_bins = cut(learn_trial_n, breaks = 5))%>%
  mutate(trial_bins = as.numeric(as.factor(trial_bins)))%>%
  group_by(ID_num, trial_bins, reward_list) %>%
  summarise(p_optimal = sum(optimal_choice)/n()) %>%
  ggplot(aes(x = as.factor(trial_bins), y = p_optimal))+
  geom_boxplot()+
  geom_jitter(alpha = 0.5)+
  facet_wrap("reward_list")
```

Bayes_SMEP was the winning model from previous analysis

model-fitting on the Cerberus (rev_analysis/combined_sample_learn_fit_bayes_SMEP.r)

```{r group_diff_params}
# posterior draws extracted from fitted object
bayes_SMEP_draws <- readRDS("/Volumes/main2/studies/Context_RL/Analysis/rev_analysis/fit_obj/combined_sample_learn_fit_bayes_SMEP_draws.rds")

library(ggdist)
# make dataframes for hyperparamters
beta_mu_diff <- as.data.frame(bayes_SMEP_draws$beta_mu_diff)
colnames(beta_mu_diff) <- "beta_mu_diff_draws"
phi_mu_diff <- as.data.frame(bayes_SMEP_draws$phi_mu_diff)
colnames(phi_mu_diff) <- "phi_mu_diff_draws"
persev_mu_diff <- as.data.frame(bayes_SMEP_draws$persev_mu_diff)
colnames(persev_mu_diff) <- "persev_mu_diff_draws"

library(tidybayes)
tidybayes::hdci(beta_mu_diff$beta_mu_diff_draws, .width = 0.95)
tidybayes::hdci(phi_mu_diff$phi_mu_diff_draws, .width = 0.95)
tidybayes::hdci(persev_mu_diff$persev_mu_diff_draws, .width = 0.95)

library(ggplot2)
# make plots
beta_mu_diff %>%
  ggplot(aes(x = beta_mu_diff_draws)) +
  # point estimate = mean; quantile interval at 95%
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+ 
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlim(c(-0.06,0.06))+
  xlab("inverse temperature")+
  ylab("")+
  theme_allie()

phi_mu_diff %>%
  ggplot(aes(x = phi_mu_diff_draws)) +
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlim(c(-0.6,0.6))+
  xlab("directed exploration")+
  ylab("")+
  theme_allie()

persev_mu_diff %>%
  ggplot(aes(x = persev_mu_diff_draws)) +
  stat_halfeye(point_interval = mean_hdci, width = .95, interval_color = "red")+
  geom_vline(xintercept = 0, color = "black", linetype = "dashed", size = 1)+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank())+
  xlab("hyperparameter posterior for group difference in perserveration")+
  ylab("")+
  theme_allie()
```

we can classify choices based on model-estimated expected values and estimated uncertainties

```{r}
# model-estimated Q value 
bayes_SMEP_Q <- as.data.frame(apply(bayes_SMEP_draws$v, c(2,3), mean))
bayes_SMEP_Q <- bayes_SMEP_Q %>%
  cbind(max_Q_choice = max.col(bayes_SMEP_Q, 'first'))

# model-estimated directed exploration bonus 
bayes_SMEP_eb <- as.data.frame(apply(bayes_SMEP_draws$eb, c(2,3), mean))
colnames(bayes_SMEP_eb) <- c("eb_1","eb_2","eb_3","eb_4")
bayes_SMEP_eb <- bayes_SMEP_eb %>%
  cbind(max_eb_choice = max.col(bayes_SMEP_eb, 'first'))

bayes_SMEP_eb$learn_trial_n <- learn_df$learn_trial_n
bayes_SMEP_eb$subject <- learn_df$ID_num
bayes_SMEP_eb$condition <- as.factor(learn_df$condition)
bayes_SMEP_eb$reward_list <- as.factor(learn_df$reward_list)
  
# classify choice on each trial 
choice_classification <- bayes_SMEP_eb %>%
  cbind(choice = learn_df$chosen_door_num) %>%
  cbind(bayes_SMEP_Q) %>%
  mutate(choice_type = case_when(
    choice == max_Q_choice ~ "exploitation",
    choice == max_eb_choice ~ "directed_exploration",
    #TRUE ~ "a_random_exploration" # to make this the default ref group for multinomial logit regression
    TRUE ~ "random_exploration"
  ),choice_type = as.factor(choice_type))

# merge with learn data
learn_data_choice_classification <- merge(learn_df, choice_classification, by.x = c("ID_num","learn_trial_n"), by.y = c("subject","learn_trial_n"))

# plot
levels_order <- c("exploitation","directed_exploration","random_exploration")
choice_plot <- choice_classification %>%
  group_by(subject, condition) %>%
  count(choice_type) %>%
  ungroup() %>%
  group_by(condition) %>%
  ggplot(aes(x = factor(choice_type, level = levels_order), y = n, color = condition))+
  geom_boxplot()+
  stat_summary(fun.data=mean_se, fun.args = list(mult=1), geom="pointrange", position = position_dodge(0.75))+
  theme_allie()+
  scale_color_red_blue("Condition")+
  xlab("")+
  ylab("number of choices (out of 50 trials)")+
  scale_x_discrete(labels=c("exploitation" = "exploitation", "directed_exploration" = "directed exploration",
                              "random_exploration" = "random exploration"))+
  ggtitle("Number of Exploitation and Exploration Choices by Condition")
choice_plot

# some stats
library(mclogit)
mod <- mblogit(choice_type ~ condition + reward_list, data = choice_classification, random = ~1|subject)
summary(mod)

# pairwise comparisons
choice_summary <- choice_classification %>%
  group_by(subject,condition) %>%
  count(choice_type)

imp_exploit <- filter(choice_summary, choice_type == "exploitation") %>% filter(condition == "imperative")
int_exploit <- filter(choice_summary, choice_type == "exploitation") %>% filter(condition == "interrogative")
# normal distribution?
shapiro.test(imp_exploit$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_exploit$n, int_exploit$n, alternative = "two.sided")

imp_direxp <- filter(choice_summary, choice_type == "directed_exploration") %>% filter(condition == "imperative")
int_direxp <- filter(choice_summary, choice_type == "directed_exploration") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(int_direxp$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_direxp$n, int_direxp$n, alternative = "two.sided")

imp_randexp <- filter(choice_summary, choice_type == "random_exploration") %>% filter(condition == "imperative")
int_randexp <- filter(choice_summary, choice_type == "random_exploration") %>% filter(condition == "interrogative")
# normal distribution?
#shapiro.test(imp_randexp$n)
# no
# wilcoxon rank sum test
wilcox.test(imp_randexp$n, int_randexp$n, alternative = "two.sided")

## effect sizes
# choice_summary %>%
#   filter(choice_type == "exploitation") %>%
#   ungroup() %>%
#   wilcox_effsize(n ~ condition)

effectsize::cohens_d(n~condition, data = choice_summary %>%
  filter(choice_type == "exploitation") %>%
  ungroup())

effectsize::cohens_d(n~condition, data = choice_summary %>%
  filter(choice_type == "directed_exploration") %>%
  ungroup(), pooled_sd = F)

effectsize::cohens_d(n~condition, data = choice_summary %>%
  filter(choice_type == "random_exploration") %>%
  ungroup())
```
individual level params

```{r params_extract}
# get the point estimate (mean) for individual subject level parameters
betas <- apply(bayes_SMEP_draws$beta, 2, mean)
phis <- apply(bayes_SMEP_draws$phi, 2, mean)
persevs <- apply(bayes_SMEP_draws$persev, 2, mean)
# learning rate on each trial
Kgains <- apply(bayes_SMEP_draws$Kgain, 2, mean)

# combine with learn data
for(row in 1:nrow(learn_df)){
  learn_df$inverse_temperature[row] <- betas[learn_df$ID_num[row]]
  learn_df$directed_exploration[row] <- phis[learn_df$ID_num[row]]
  learn_df$perseveration[row] <- persevs[learn_df$ID_num[row]]
  learn_df$learning_rate[row] <- Kgains[row]
}

# prediction error on each trial is received - expected reward for the chosen door 
for(row in 1:nrow(learn_df)){
  learn_df$prediction_error[row] <- learn_df$reward[row] - bayes_SMEP_Q[row, learn_df$chosen_door_num[row]]
}

# append choice type
learn_df$choice_type <- learn_data_choice_classification$choice_type

# export 
write.csv(learn_df, file = "/Users/candiceyuxiwang/Documents/CNAP/Projects/contextRL/combined_sample_learn_data_params.csv")
```

